{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3ee93e-655c-4c55-8047-0f8603b7e9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0977f52-af31-438a-98a9-60c3dfbc398f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('C:/Users/USER/Desktop/AI Lab/Data/01-Data/lena.png') #opencv images are basically numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070e62a-0a36-462f-b0a0-2d9fe2e59f56",
   "metadata": {},
   "source": [
    "If you have the alpha channel, opencv drops the last channel. it reads the image with only 3 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4a986c-5f33-45e1-b9c5-0367ea4ab401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_img = img[:150, :150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0d3b48-9e77-4200-aacb-69dfb544cb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"Loaded image\", sub_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003be20f-8856-48f5-bdbb-dcdf9a22217e",
   "metadata": {},
   "source": [
    "## Drawing stuff inside an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1cb66e3-bf59-4355-abaa-75eecab14e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b64e2b-63e9-4f37-a57a-2064516eb86c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81e9840-fbcc-4242-a2d2-e30420cc14f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "canvas = np.zeros((300,300,3), dtype=np.uint8) #always unsigned 8 bit integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cacab2b6-8031-46e7-99f7-81f8cd7725b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drawing a green line.\n",
    "# define green color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65d48a5e-58c1-47e1-94ed-27f705681b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "green = (0,255,0) #remember it is BGR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c5a010-f9d2-43ac-a176-47069456ea34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#draw the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f340479-d86d-47fa-810e-95520e5f0944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.line(canvas,(50,50),(100,100),green, 10) #10 is the thickness of the color.\n",
    "cv2.imshow(\"Draw\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() #to fix the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa445ba-d99f-4478-9709-14d83c8cd999",
   "metadata": {},
   "source": [
    "Here we get a black image. Why? Because the canvas has just one channel.\n",
    "A one channel image is a grayscale image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bcca71-3c8f-421c-8eaa-bda0f3ae4ef9",
   "metadata": {},
   "source": [
    "We need to create a 3 channel image. We fix it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e93e19f-4bc4-4757-a391-79e83f80f30d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "canvas = np.zeros((300,300,3), dtype=np.uint8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed89779-24b2-42a8-885c-ac45fc4e3c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#draw the rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8289be8d-91b5-4204-8332-7fc27fe6055d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "red = (0,0,255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d11b7d-a683-47d3-819b-3cd972b8a406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.rectangle(canvas, (150,150), (200,200),red,-1) #top left and bottom right coordinates. thickness -1 means you want to fill the rectangle.\n",
    "cv2.imshow(\"Draw\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() #to fix the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8098921e-7499-4ee8-9c52-12072cd7f0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define color for the circle\n",
    "blue = (255,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb33f6e-19f6-47df-b894-89faa4193c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = canvas.shape[1] // 2  #do integer division!\n",
    "y = canvas.shape[0] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edc67ad9-8ed5-454b-bc8f-cb4e29778868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.circle(canvas, (x,y), 10, blue, -1)\n",
    "cv2.imshow(\"Draw\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() #to fix the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a36e2b-8366-4756-9552-d8c67d9f32e2",
   "metadata": {},
   "source": [
    "## Manipulating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ba060e9-c207-4195-9a03-10e3232483f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b08b13-3b81-4090-8a76-b3db0bde56a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('C:/Users/USER/Desktop/AI Lab/Data/01-Data/lena.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2da77977-e4a4-4b9b-8a26-88abf1a7c173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b,g,r = cv2.split(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16cb6f95-f5c1-4b79-baf0-97ba49a3748b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imshow(\"blue\",b)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eef6b099-20fa-436c-a559-4cc85c0456a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imshow(\"green\",g)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81d93f95-5e18-42f5-9a99-e698f7617cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imshow(\"red\",r)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "661ce122-4ddb-4e21-bae5-625704af5b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#neater way\n",
    "channels = np.hstack([b,g,r])\n",
    "cv2.imshow(\"stacked\",channels)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c70f9-78a0-4bf2-90e6-0ef30f895d06",
   "metadata": {},
   "source": [
    "The darker are the pixles of a channel, the less that color contributes to that pixel and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530d3a1-2bb8-4885-9f54-ca8e0e63583d",
   "metadata": {},
   "source": [
    "In the Lena example, the red dominates so the red channel is lighter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3481aa21-972f-41f7-8003-fcb250165d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# considering it as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8681501-fb11-4fc3-b97f-60045d894156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = img[:,:,0]\n",
    "g = img[:,:,1]\n",
    "r = img[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a17ec361-0725-4a2e-9939-85d5ee7fec57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# numpy approach is faster! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ac11b-e5ab-4afe-b24c-52d35c870af3",
   "metadata": {},
   "source": [
    "## How to merge channels together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e4bb28ec-e42b-4b92-8f5b-3b6df7ca5e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_img = cv2.merge((r,g,b)) #just like matplotlib!\n",
    "cv2.imshow(\"oop\", new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "646b7196-9986-4090-8600-faae60eb7079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_img = cv2.merge((b,g,r))\n",
    "cv2.imshow(\"oop\", new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07ffbc67-8946-4a30-8f92-713a1d15fe61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85136bf6-318e-4e53-8949-60484e77c322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.dstack() stacks based on depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3a54b-3b81-4aaf-89c3-13cc7d014626",
   "metadata": {},
   "source": [
    "## Geometrical transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4083b-7b64-4c77-8c7d-7fa18d23fe7f",
   "metadata": {},
   "source": [
    "Data augmentation -> transformations to make our data bigger. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b26a6e-102b-4899-a857-c20bc4111ea0",
   "metadata": {},
   "source": [
    "### 1st type: Affine transformation (preserving parallel lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b58a2d-28c3-480b-a96c-3bcc60f548da",
   "metadata": {},
   "source": [
    "Affine vs perspective. \n",
    "Geometric transformations are represented as matrices. \n",
    "An affine transformation is 2x3 matrix.\n",
    "A perspective is a 3x3 matrix.\n",
    "The **perspective transformation** you do not have paralle lines preserved. In **affine transformations**, the parallel lines are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bbe6422-2c57-4969-844a-26560899b70f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a6c13c07-28e7-4871-973d-aa1e9aabb0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('C:/Users/USER/Desktop/AI Lab/Data/01-Data/lena.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f20b8c2-5d65-4857-a71e-5fa700146421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resized_img = cv2.resize(img,(200,200), interpolation=cv2.INTER_LINEAR)\n",
    "# the interpolation method is the fuction that is used to fill the gap when you enlarge the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cd77c0c-a862-431f-a21d-54980b2f5488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imshow(\"Transformed\", resized_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf65bd1-029f-4960-bd90-4d320261730e",
   "metadata": {},
   "source": [
    "The interpolation algorithm is the algorithm that fills the missing data. When moving to smaller resolutions the interpolation is not needed because we have the data. If we want to **upscale** our image, the missing data shall be put. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc3b36-df3a-4e30-8f96-9f7a19d49bf2",
   "metadata": {},
   "source": [
    "DeepLearning models are used today to do the upscaling correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26abdd5-351f-40e2-90d9-faa9c8e10500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resized_img = cv2.resize(img,None, fx=0.1, fy=0.5, interpolation=cv2.INTER_LINEAR)\n",
    "# fx, fy tell ab scaling in x,y axis respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26a25a5e-0be0-4a6f-ae99-6892bc6385a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imshow(\"Transformed\", resized_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850dabe-591b-4630-a099-b87e36075300",
   "metadata": {},
   "source": [
    "#### Translation matrix\n",
    "Is a way to move the image around. (informally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4989b552-802e-44ed-9014-9f714a48b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nIs an affine transformation.(2x3)\\n1 0 tx\\n0 1 ty\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Is an affine transformation.(2x3)\n",
    "1 0 tx\n",
    "0 1 ty\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "020ce5e0-0c16-49eb-ada4-8729647d53d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = np.float32([\n",
    "    [1,0,200],\n",
    "    [0,1,50]]) #everything in computation shall be float32\n",
    "# tx,ty = 200, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54477f31-8146-4f0e-8f75-dcce61daefbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply the matrix to our image\n",
    "h,w = img.shape[:2]\n",
    "new_img = cv2.warpAffine(img, M, (w,h)) #maps each pixel to a destination point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc2f35eb-840c-4d6a-9c32-baf7116529d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imshow(\"Transformed\", new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \n",
    "# THE IMAGE is moved 200 to the right and 50 down. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c693dd-eeae-4367-a31c-351c9bf9bbb8",
   "metadata": {},
   "source": [
    "If we make (w,h) larger the black su"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
