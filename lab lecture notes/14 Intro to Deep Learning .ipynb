{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660cad72-bdd5-44ce-ad4e-5583db7bc418",
   "metadata": {},
   "source": [
    "# How do neural networks work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a7856-4d53-459e-9bd6-1a939d16efdb",
   "metadata": {},
   "source": [
    "With a single perceptron we can only learn linear functions. To fix this we use more than 1 perceptron a.k.a MLP / ANN.  \n",
    "Fully connected layers also refer to MLP (they're the same).  \n",
    "\n",
    "**The output layer**: 1 node for each class we want to classify our input into.  \n",
    "Each node has some probability of belonging to that class.  \n",
    "We pick the node with the highest probability as our class.  \n",
    "If you're doing binary classification one node is enough.\n",
    "\n",
    "**The input layer:** 1 node for each feature of our input.  \n",
    "For a NXN image we need NXN nodes. One node for each pixel of our image.  \n",
    "\n",
    "**The hidden layer**: Is arbitrary.  \n",
    "You should change hidden layer size if model is overfitting or not learning.  \n",
    "\n",
    "\n",
    "\n",
    "*Feedforward networks:* The input goes only one way. It does not go back, no cycles.  \n",
    "*Recurrent networks:* The input goes in a cycle.   \n",
    "\n",
    "REMEMBER: During training the loss function should go to 0 and the accuracy to 1.   \n",
    "\n",
    "Take the error and tell each node to change the weight as so to minimize the error. (propagation)   \n",
    "You start from the last layer onto the first one. (back)  \n",
    "This is called **backpropagation**.\n",
    "\n",
    "Computing derivates is expensive so the **stochastic gradient descend** is used.  \n",
    "Remember batch size, used since it is not feasible to load all data set in our machine memory. \n",
    "\n",
    "This is done for each sample of our dataset!    \n",
    "\n",
    "\n",
    "*Why is scaling important for neural networks?*  \n",
    "They are very sensitive to scaling issues! For example imagine your features are height and foot size in cm  \n",
    "clearly they have different magnitudes but the model thinks the height is a more important feature!  \n",
    "Thus we scale each feature between 0 and 1.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a8709-4a01-4e1b-ace1-057463451cd1",
   "metadata": {},
   "source": [
    "## Multilayer perceptron in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baff055-d2ce-42ec-80f8-cc3c9a44be5f",
   "metadata": {},
   "source": [
    "The training and the testing set is to be done manually in pytorch.  \n",
    "You can simply copy and paste that training loop in each pytorch program since 99% of the time it is the same.  \n",
    "The standard data type for a pytorch is a tensor.  \n",
    "Tensors are numpy arrays that can be moved onto GPU. Think of it as an upgrade of numpy arrays. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a3c879-5a30-4c3f-b971-e0428d888582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision import datasets #datasets to comp vision tasks\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn #contains info ab all possible layers we need to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3baf18a-3d19-4d85-9fb4-ae3f7827d228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
